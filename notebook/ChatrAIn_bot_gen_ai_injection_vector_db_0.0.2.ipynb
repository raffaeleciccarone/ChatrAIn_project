{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "945bfa1f",
   "metadata": {},
   "source": [
    "**FIRST PART-PREPROCESSING,EMBEDDING,VECTORDB**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223295f0",
   "metadata": {},
   "source": [
    "*IMPORT*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72d6b041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait for import...\n",
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "print('Wait for import...')\n",
    "import os\n",
    "import glob\n",
    "import fitz #to import pdf\n",
    "import re\n",
    "import chromadb #for vectordb\n",
    "from chromadb.config import Settings\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import ollama\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "print('Ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6913830",
   "metadata": {},
   "source": [
    "*FUNCTIONS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3bfc950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "        print('mps ready')\n",
    "    else:       \n",
    "        device = 'cpu'\n",
    "        print('=( no mps')\n",
    "    return device\n",
    "\n",
    "def path_extraction(path_manuals):\n",
    "    #the return is not a sorted list\n",
    "    return glob.glob(os.path.join(path_manuals, '*.pdf')) #if you want: sorted(glob.gl.....)\n",
    "\n",
    "def cleaning(text):\n",
    "    #minimal pre-cleaning\n",
    "    text = text.replace('\\x00', '') #ok for db\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def chunkin(text):\n",
    "    clean_text = cleaning(text)\n",
    "    #langchain\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len, \n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"] \n",
    "    )\n",
    "    \n",
    "    return text_splitter.split_text(clean_text)\n",
    "\n",
    "def text_preprocessing(path_manuals):\n",
    "    pdf_files = path_extraction(path_manuals)\n",
    "    all_chunks = []\n",
    "    all_metadatas = []\n",
    "\n",
    "    print(f\"Processing {len(pdf_files)} files...\")\n",
    "\n",
    "    for pdf_path in tqdm(pdf_files, desc=\"Lettura PDF\"):\n",
    "        full_text = ''\n",
    "        file_name = os.path.basename(pdf_path)\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                full_text += page.get_text()\n",
    "            #BLOCK CHUNCKING\n",
    "            doc_chunks = chunkin(full_text)\n",
    "            #METADATA SAVE\n",
    "        for c in doc_chunks:\n",
    "            all_chunks.append(c)\n",
    "            all_metadatas.append({'source': file_name})\n",
    "            #lock loop and divide documents\n",
    "    \n",
    "\n",
    "    if len(all_chunks) != len(all_metadatas):\n",
    "        print(f\"WARNING! Chunks: {len(all_chunks)}, Metas: {len(all_metadatas)}\")\n",
    "        return [], []\n",
    "\n",
    "    print(f\"Extraction {len(all_chunks)} total chunks from {len(pdf_files)} files COMPLETE.\")\n",
    "    return all_chunks, all_metadatas\n",
    "\n",
    "def create_vector_db(db_path, collec_name, embedding_model, chunks, metadatas, batch_size=500):\n",
    "    client = chromadb.PersistentClient(path=db_path)\n",
    "    '''\n",
    "    if you want to make many attempts, unlock this part of the code.\n",
    "    \n",
    "    try:\n",
    "        client.delete_collection(collec_name)\n",
    "    except:\n",
    "        pass\n",
    "    '''\n",
    "    collection = client.get_or_create_collection(name=collec_name)\n",
    "\n",
    "    tot_chunk_n = len(chunks)\n",
    "    print(f\"Generating embeddings and saving {tot_chunk_n} chunks...\")\n",
    "\n",
    "    #process chunks in batches for Out Of Memory\n",
    "    for i in range(0, tot_chunk_n, batch_size):\n",
    "        \n",
    "        end_index = min(i + batch_size, tot_chunk_n)\n",
    "\n",
    "        batch_chunks = chunks[i : end_index]\n",
    "        batch_metas = metadatas[i : end_index]\n",
    "        batch_ids = [f\"id_{k}\" for k in range(i, end_index)] # generate unique id\n",
    "        \n",
    "        encoded_output = embedding_model.encode(batch_chunks)\n",
    "        batch_embeddings = encoded_output['dense_vecs']\n",
    "\n",
    "        # Append to the database\n",
    "        collection.add(\n",
    "            ids=batch_ids,\n",
    "            embeddings=batch_embeddings,\n",
    "            documents=batch_chunks,\n",
    "            metadatas=batch_metas\n",
    "        )\n",
    "        \n",
    "    print(f\"Done! {len(chunks)} vettors saved into: '{db_path}'.\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9624cd5",
   "metadata": {},
   "source": [
    "**SET VARIABLES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62610251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 7 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lettura PDF: 100%|██████████| 7/7 [00:03<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction 7223 total chunks from 7 files COMPLETE.\n",
      "7223 ready!\n",
      "mps ready\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f46525d8244e22abe4f30ac3671757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model: BGE-M3\n",
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "#PATH\n",
    "PDF_PATH = 'new_manuals'\n",
    "DB_PATH = \"/Users/raffaeleciccarone/Desktop/project_ibm/chroma_db\"         \n",
    "COLLECTION_NAME = \"manuali_manutenzione_test\"\n",
    "#MAKE CHUNKING\n",
    "all_chunks, all_metadatas = text_preprocessing(PDF_PATH)\n",
    "print(f'{len(all_chunks)} ready!')\n",
    "#DEFINE EMBEDDING MODEL\n",
    "model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True, devices=device())\n",
    "print(\"Embedding model: BGE-M3\")\n",
    "print(\"Ready!\")\n",
    "#BATCH SIZE\n",
    "BATCH_SIZE = 500 #5461 max size for local sqllite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ab243ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings and saving 7223 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pre tokenize: 100%|██████████| 2/2 [00:00<00:00,  6.62it/s]\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Inference Embeddings: 100%|██████████| 2/2 [07:40<00:00, 230.44s/it]\n",
      "pre tokenize: 100%|██████████| 2/2 [00:00<00:00,  5.82it/s]\n",
      "Inference Embeddings: 100%|██████████| 2/2 [03:02<00:00, 91.09s/it] \n",
      "pre tokenize: 100%|██████████| 2/2 [00:00<00:00, 13.08it/s]\n",
      "Inference Embeddings: 100%|██████████| 2/2 [14:07<00:00, 423.91s/it]\n",
      "pre tokenize: 100%|██████████| 2/2 [00:00<00:00,  5.56it/s]\n",
      "Inference Embeddings: 100%|██████████| 2/2 [20:21<00:00, 610.75s/it]\n",
      "pre tokenize: 100%|██████████| 2/2 [00:00<00:00,  2.25it/s]\n",
      "Inference Embeddings: 100%|██████████| 2/2 [21:24<00:00, 642.28s/it] \n",
      "pre tokenize: 100%|██████████| 2/2 [00:00<00:00, 16.27it/s]\n",
      "Inference Embeddings: 100%|██████████| 2/2 [11:48<00:00, 354.04s/it]\n",
      "pre tokenize: 100%|██████████| 2/2 [00:00<00:00, 13.89it/s]\n",
      "Inference Embeddings: 100%|██████████| 2/2 [00:42<00:00, 21.41s/it]\n",
      "pre tokenize: 100%|██████████| 2/2 [00:00<00:00, 43.59it/s]\n",
      "Inference Embeddings: 100%|██████████| 2/2 [00:38<00:00, 19.25s/it]\n",
      "pre tokenize: 100%|██████████| 2/2 [00:00<00:00, 23.22it/s]\n",
      "Inference Embeddings: 100%|██████████| 2/2 [00:35<00:00, 17.67s/it]\n",
      "pre tokenize: 100%|██████████| 2/2 [00:00<00:00, 12.34it/s]\n",
      "Inference Embeddings: 100%|██████████| 2/2 [03:16<00:00, 98.06s/it] \n",
      "pre tokenize: 100%|██████████| 2/2 [00:00<00:00,  9.31it/s]\n",
      "Inference Embeddings: 100%|██████████| 2/2 [06:56<00:00, 208.04s/it]\n",
      "pre tokenize: 100%|██████████| 2/2 [00:00<00:00, 16.17it/s]\n",
      "Inference Embeddings: 100%|██████████| 2/2 [09:58<00:00, 299.16s/it]\n",
      "pre tokenize: 100%|██████████| 2/2 [00:00<00:00, 13.57it/s]\n",
      "Inference Embeddings: 100%|██████████| 2/2 [18:37<00:00, 558.90s/it] \n",
      "pre tokenize: 100%|██████████| 2/2 [00:00<00:00, 11.34it/s]\n",
      "Inference Embeddings: 100%|██████████| 2/2 [17:20<00:00, 520.24s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! 7223 vettors saved into: '/Users/raffaeleciccarone/Desktop/project_ibm/chroma_db'.\n"
     ]
    }
   ],
   "source": [
    "create_vector_db(DB_PATH, COLLECTION_NAME, model, all_chunks, all_metadatas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c158b5c",
   "metadata": {},
   "source": [
    "**TEST INFO | retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a9118d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User query: 'Come si esegue la manutenzione del sistema di frenata?'\n",
      "\n",
      "=== TOP 5 RESULTS ===\n",
      "\n",
      "CIUUUF CIUUUUUUF n. #1 (distance: 0.6472)\n",
      "From: ETR_500.pdf\n",
      "Chunk text: la pressione residua all'interno dell'impianto non possa provocare infortuni.\n",
      "—\n",
      "Scaricare l'aria dell'impianto.\n",
      "—\n",
      "Aprire, con la chiave in dotazione al personale, lo sportello dei serbatoi freno (fig....\n",
      "=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D\n",
      "\n",
      "CIUUUF CIUUUUUUF n. #2 (distance: 0.6542)\n",
      "From: ETR_500.pdf\n",
      "Chunk text: la pressione residua all'interno dell'impianto non possa provocare infortuni.\n",
      "—\n",
      "Scaricare l'aria dell'impianto.\n",
      "—\n",
      "Aprire, con la chiave in dotazione al personale, lo sportello in cui è ubicato il grup...\n",
      "=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D\n",
      "\n",
      "CIUUUF CIUUUUUUF n. #3 (distance: 0.6693)\n",
      "From: MR1-3-A.pdf\n",
      "Chunk text: 1.2. \n",
      "Funzionamento generale dell’impianto freno\n",
      "Il sistema frenante prevede:\n",
      "-\n",
      "la frenatura elettrodinamica (dalla velocità massima sino alla velocità di 5 km/h) di tipo\n",
      "reostatico ed a recupero;\n",
      "-\n",
      "l...\n",
      "=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D\n",
      "\n",
      "CIUUUF CIUUUUUUF n. #4 (distance: 0.6696)\n",
      "From: ETR_500.pdf\n",
      "Chunk text: riodicamente sulle varie parti che compongono l'impianto pneumatico e freno, per garan-\n",
      "tire nel tempo la loro perfetta funzionalità.\n",
      "Tutte le operazioni dovranno essere effettuate nell'ordine cronolo...\n",
      "=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D\n",
      "\n",
      "CIUUUF CIUUUUUUF n. #5 (distance: 0.6805)\n",
      "From: ETR_500.pdf\n",
      "Chunk text: —\n",
      "Scaricare l'aria dell'impianto.\n",
      "—\n",
      "Aprire, con la chiave in dotazione al personale, lo sportello in cui è ubicato il gruppo\n",
      "freno (fig. 1, pos. 1).\n",
      "—\n",
      "Procedere alla pulizia del filtro (2).\n",
      "NOTA: Per ...\n",
      "=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D=D\n"
     ]
    }
   ],
   "source": [
    "def retrieve_documents(query, embedding_model, collection, n_results=3):\n",
    "    \"\"\"\n",
    "    Research into vector DB.\n",
    "    Return text, metadatas and score.\n",
    "    \"\"\"\n",
    "    print(f\"\\nUser query: '{query}'\")\n",
    "\n",
    "    encoded_output = embedding_model.encode(query)\n",
    "    query_vector = encoded_output['dense_vecs']\n",
    "\n",
    "    # WARNING: query_embeddings want a list\n",
    "    # So: [query_vector]\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_vector],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"] \n",
    "    )\n",
    "\n",
    "    print(f\"\\n=== TOP {n_results} RESULTS ===\")\n",
    "    \n",
    "    retrieved_docs = results['documents'][0]\n",
    "    retrieved_metas = results['metadatas'][0]\n",
    "    retrieved_dists = results['distances'][0]\n",
    "\n",
    "    for i, (doc, meta, dist) in enumerate(zip(retrieved_docs, retrieved_metas, retrieved_dists)):\n",
    "        print(f\"\\nCIUUUF CIUUUUUUF n. #{i+1} (distance: {dist:.4f})\")\n",
    "        print(f\"From: {meta.get('source', 'Unknown')}\") \n",
    "        print(f\"Chunk text: {doc[:200]}...\") \n",
    "        print(\"=D\" * 50)\n",
    "\n",
    "    # Entire pack for llm\n",
    "    return results\n",
    "\n",
    "\n",
    "query_user = \"Come si esegue la manutenzione del sistema di frenata?\"\n",
    "client = chromadb.PersistentClient(path=DB_PATH)\n",
    "collection = client.get_collection(name=COLLECTION_NAME)\n",
    "\n",
    "context_results = retrieve_documents(\n",
    "    query=query_user,\n",
    "    embedding_model=model,  \n",
    "    collection=collection,  \n",
    "    n_results=5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
